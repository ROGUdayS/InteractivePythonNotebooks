{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234cc4de",
   "metadata": {},
   "source": [
    "# Advanced Classification using Machine Learning in Health Care (Diagnosis of Covid using chest X-rays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b852c978",
   "metadata": {},
   "source": [
    "Project done by Uday Kumar S\n",
    "\n",
    "Roll no. 20ETMC412015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b5338",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78990c96",
   "metadata": {},
   "source": [
    "X-rays are widely used in Medical Practice. They can be used to identify various diseases. However, a diagnosis depends on a doctor's experience, which can lead to improper treatment. Modern methods of Artificial Intelligence and pattern recognition make it possible to create expert systems that allow you to establish a diagnosis automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c3735",
   "metadata": {},
   "source": [
    "There are two approaches we can take for the classification of images :\n",
    "\n",
    "    1. Different classical methods and their comparison\n",
    "    2. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e57d9",
   "metadata": {},
   "source": [
    "## Approach taken in this program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0c138",
   "metadata": {},
   "source": [
    "1. Download and preliminary transormation of images\n",
    "2. Creating Image features\n",
    "3. Comparing differnent classical classification methods\n",
    "4. Building and fitting of Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae97300",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a6523",
   "metadata": {},
   "source": [
    "* Download and transform images.\n",
    "* Create features of images.\n",
    "* Build different classification models.\n",
    "* Build CNN models.\n",
    "* Build a diagnosis based on X-ray photos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad264d99",
   "metadata": {},
   "source": [
    "## Packages Required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c2e8f",
   "metadata": {},
   "source": [
    "* Python\n",
    "* OS\n",
    "* Numpy\n",
    "* Glob\n",
    "* SeaBorn\n",
    "* Matplotlib\n",
    "* Mahotas\n",
    "* Keras\n",
    "* Scikit-learn\n",
    "* Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bdcb69",
   "metadata": {},
   "source": [
    "### Required libraries installation commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffab704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617b06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5437c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046e89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mahotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab4dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install os-sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47f6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06fe1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "931cacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716c5d54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#conda install -c conda-forge tensorflow --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20e70b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950eb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c apple tensorflow-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0235b39a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f70739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a892879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge keras --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987049b",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90f449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mahotas as mh\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e71a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b3c016",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/Users/rogudays/anaconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscriminant_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuadraticDiscriminantAnalysis\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/Users/rogudays/anaconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "#Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca18f1",
   "metadata": {},
   "source": [
    "## Importing the Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e3d68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f05584011d4a648d658b591cfc678b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading Covid19-dataset.zip:   0%|          | 0/165693838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3a889b762845d18d5173c124490314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410d6659b03b4c7e8f23796ba5019c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading NN.zip:   0%|          | 0/23835432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d758731c179a4536b07c28105a8991a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import skillsnetwork\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/data-science-in-health-care-advanced-machine-learning-classification/LargeData/Covid19-dataset.zip\", overwrite=True)\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/data-science-in-health-care-advanced-machine-learning-classification/NN.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f626702",
   "metadata": {},
   "source": [
    "Images have to be of the same size for classification. In order to achieve this, let's create a global variable that will determine the size (height and width) for image resizing. Both are 224 in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMM_SIZE=244"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffc112",
   "metadata": {},
   "source": [
    "For convenience, we create a function that downloads and displays all the pictures from a specified directory. In order to classify images, all pictures should be placed in sub-directories. The names of these subdirectories are actually class names. In our case, the images are X-rays which should be placed into subfolders with the names of diagnose. First of all, we should create a list of subfolders, which is a list of possible disease classes. In our case, \n",
    "it will be:\n",
    "* Normal\n",
    "* COVID\n",
    "* Pneumonia\n",
    "\n",
    "Next we need to create a list of all images. After that, let's download and preprocess all the images:\n",
    "\n",
    "1. Download by mahotas.imread()\n",
    "2. It is necessary to resize the images to (IMM_SIZE x IMM_SIZE). If an image is gray-colored it is presented as a 2D matrix: (height, width).jpg and png images are 3D (height, width, 3 or 4). To do this, we should use: mahotas.resize_to()\n",
    "3. If the third parameter of an image shape equals 4, it means alpha channel. We can remove it using slice image[:,:,:3].\n",
    "4. Then we need to transform all the images to gray 2D format by: mahotas.colors.rgb2grey()\n",
    "\n",
    "The function returns an array of tuples[image, class name]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe31556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    class_names=[f for f in os.listdir(folder) if not f.startswith('.')] # create a list of subfolders\n",
    "    data=[]\n",
    "    # class_names = ['Covid','Normal','Viral Pneumonia']\n",
    "    print(class_names)\n",
    "    for t, f in enumerate(class_names):\n",
    "        images = glob(folder + \"/\" + f + \"/*\") # Create a list of files\n",
    "        print('Downloading: ',f)\n",
    "        fig=plt.figure(figsize=(50,50))\n",
    "        for im_n, im in enumerate(images):\n",
    "            plt.gray() # set gey colormap of images\n",
    "            image=mh.imread(im)\n",
    "            if len(image.shape)>2:\n",
    "                image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE, image.shape[2]]) # resize of RGB and png images\n",
    "            else:\n",
    "                image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE]) # resize of grey images\n",
    "            if len(image.shape)>2:\n",
    "                image = mh.colors.rgb2grey(image[:,:,:3], dtype=np.uint8) #change of colormap of images alpha channel delete\n",
    "            plt.subplot(int(len(images)/5)+1,5,im_n+1) # Create a table of images\n",
    "            plt.imshow(image)\n",
    "            data.append([image, f])\n",
    "        plt.show()\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d163e4",
   "metadata": {},
   "source": [
    "For training and testing, all the pictures should be divided into training and test groups and located in separate folders. Let's upload all the images to the corresponding arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "d='/Users/rogudays/Downloads/Covid19-dataset/train'\n",
    "train=get_data(d)\n",
    "\n",
    "d='/Users/rogudays/Downloads/Covid19-dataset/test'\n",
    "val=get_data(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ba303",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Shape', train.shape) # Size of the training DataSet\n",
    "print('Test Shape', val.shape) # Size of the test DataSet\n",
    "print('Image Size', train[0][0].shape) # Size of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed2e43",
   "metadata": {},
   "source": [
    "As we can see, the training DataSet consists of 251 images and the test one consists of 66 images. All the images are in grey 2D (244x244) format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01156109",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b13d7f",
   "metadata": {},
   "source": [
    "Let's visualize our data and see what exactly are we working with. We use Seaborn to plot the number of images in all the classes. You can see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17885e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in train:\n",
    "    l.append(i[1])\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11c0f8",
   "metadata": {},
   "source": [
    "let's also visualize the first image from the Viral Pneumonia, Normal and Covid classes in the training DataSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79840651",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(train[np.where(train[:,1]=='Viral Pneumonia')[0][0]][0])\n",
    "plt.title('Viral Pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(train[np.where(train[:,1]=='Covid')[0][0]][0])\n",
    "plt.title('Covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(train[np.where(train[:,1]=='Normal')[0][0]][0])\n",
    "plt.title('Normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa042325",
   "metadata": {},
   "source": [
    "## Image Features Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd43390",
   "metadata": {},
   "source": [
    "To classify objects, you need to transform the data set so that the input is a set of features and the output is an object class. An image is a matrix of pixels. Each pixel is a color. Therefore, it is impossible to submit a bare picture into a classical classifier's input. It is necessary to turn each picture into a set of certain features.\n",
    "\n",
    "Mahotas makes it easy to calculate features of an image. The corresponding functions are found in the (mahotas.features) submodule. The Haralick set of texture features are based on textures, that is, they distinguish between structured and unstructured images, as well as various repetitive structures. With the help of Mahotas, these features are calculated very easily: haralick_features = mh.features.haralick(image) haralick_features_mean=np.mean(haralick_features, axis=0) haralick_features_all=np.ravel(haralick_features) The mh.features.haralick function returns a 4 x 13 array which should be transformed into 1D by NumPy.ravel(). The first dimension is the four possible directions in which the features are calculated (vertical, horizontal, and two diagonals). If we are not interested in any particular direction, then we can average the features in all directions. Alternatively, you can use all the characteristics individually(variable haralick_features_all). The choice depends on the properties of a particular dataset. We decided that the vertical and horizontal featues should be stored separately in our case, so we use haralick_features_all.\n",
    "\n",
    "We should make a function for the DataSet features creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    for image, label in data:\n",
    "        features.append(mh.features.haralick(image).ravel())\n",
    "        labels.append(label)\n",
    "    features=np.array(features)\n",
    "    labels=np.array(labels)\n",
    "    return(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, labels_train = create_features(train)\n",
    "features_test, labels_test = create_features(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ac7df",
   "metadata": {},
   "source": [
    "## Comparing different classical classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32a998",
   "metadata": {},
   "source": [
    "If we want to compare some classificators, we should use a Pipeline. A pipeline helps to chain multiple estimators into a single one. This is useful as there is often a fixed number of steps in data processing, for example, feature selection, normaliztion and classification. A pipeline serves multiple purposes here:\n",
    "\n",
    "You only have to call fit() once to evaluate a whole sequence of estimators. \n",
    "\n",
    "You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "Pipelines help to avoid leaking statistics from your test data into the trained model in cross-validation by ensuring that the same samples are used to train the transformers and predictors. All estimators in a pipeline, except the last one, should be transformers (i.e. should have a transform method). The last estimator may be of any type (transformer, classifier, etc).\n",
    "\n",
    "The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators.\n",
    "\n",
    "In order to test how it works, we will use LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('preproc', StandardScaler()), ('classifier', LogisticRegression())])\n",
    "clf.fit(features_train, labels_train)\n",
    "scores_train = clf.score(features_train, labels_train)\n",
    "scores_test = clf.score(features_test, labels_test)\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))\n",
    "plot_confusion_matrix(clf, features_test, labels_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2f1c2",
   "metadata": {},
   "source": [
    "As we can see, the results are not bad. Conflusion matrix shows us how many mistaken predictions we got. it allows us to check other classifiers and compare the results.\n",
    "\n",
    "We will test:\n",
    "\n",
    "* Logistic Regression\n",
    "* Nearest Neighbours\n",
    "* Linear SVM\n",
    "* RBF SVM\n",
    "* Gaussian Process\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Multi-layer Perceptron classifier\n",
    "* Ada Boost\n",
    "* Naive Bayes\n",
    "* Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(\"Fitting:\", name)\n",
    "    clf = Pipeline([('preproc', StandardScaler()), ('classifier', clf)])\n",
    "    clf.fit(features_train, labels_train)\n",
    "    score_train = clf.score(features_train, labels_train)\n",
    "    score_test = clf.score(features_test, labels_test)\n",
    "    scores_train.append(score_train)\n",
    "    scores_test.append(score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331a533",
   "metadata": {},
   "source": [
    "Let's print the results as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(index=names)\n",
    "res['scores_train']=scores_train\n",
    "res['scores_test']=scores_test\n",
    "res.columns=['Test', 'Train']\n",
    "res.index.name='Classifier accuracy'\n",
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c63c4e2",
   "metadata": {},
   "source": [
    "You can see that the calculations are very fast and that Logistic Regression and Neural Network show the best result for the test DataSet.\n",
    "\n",
    "Let's compare the results on a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(names)) # the label locations\n",
    "width = 0.35 # the width of the bars\n",
    "\n",
    "fig, ax =plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, scores_train, width, label = 'Train')\n",
    "rects2 = ax.bar(x + width/2, scores_train, width, label = 'Test')\n",
    "\n",
    "# Add some test for labels, title, and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of classifiers')\n",
    "ax.set_xticks(x)\n",
    "plt.xticks(rotation = 90)\n",
    "ax.set_xticklabels(names)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b20b81",
   "metadata": {},
   "source": [
    "## Building and fitting Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080391b2",
   "metadata": {},
   "source": [
    "### Required libraries import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4c007",
   "metadata": {},
   "source": [
    "We will use Keras library for creating and training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a385759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474cc3fc",
   "metadata": {},
   "source": [
    "## Data preprocessing and data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931249e0",
   "metadata": {},
   "source": [
    "What is different about Convolutional Neural Networks is that we can submit images directly to the input. However, these images also equire pre-processing.\n",
    "\n",
    "In particular, it is necessary to normalize the pixel color, i.e. to normalize them from the range [0, 255) to [0,1).\n",
    "\n",
    "You also need to change the dimension of the input images because of Keras framework.\n",
    "\n",
    "Image classes should be of numeric type instead of string.\n",
    "\n",
    "The code below makes the necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba755723",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "x_val=[]\n",
    "y_val=[]\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "    \n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)\n",
    "    \n",
    "# Normalize the data\n",
    "x_train = np.array(x_train)/255\n",
    "x_val = np.array(x_val)/255\n",
    "\n",
    "# Reshaping input images\n",
    "x_train = x_train.reshape(-1, IMM_SIZE, IMM_SIZE, 1)\n",
    "x_val = x_val.reshape(-1, IMM_SIZE, IMM_SIZE, 1)\n",
    "\n",
    "# Creating a dictionary of clases\n",
    "lab = {}\n",
    "for i, l in enumerate(set(y_train)):\n",
    "    lab[l] = i\n",
    "    \n",
    "y_train = np.array([lab[l] for l in y_train])\n",
    "y_val = np.array([lab[l] for l in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the input DataSet :', x_train.shape)\n",
    "print('Shape of the output DataSet :', y_train.shape)\n",
    "print('Dictionary of classes :', lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694d0e7",
   "metadata": {},
   "source": [
    "## Data augmentation on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb43431",
   "metadata": {},
   "source": [
    "We should perform data augmentation to fit our model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center = False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center = False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization = False,  # divide each input by its std\n",
    "    zca_whitening = False,  # apply ZCA whitening\n",
    "    rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.2,  # Randomly zoom image\n",
    "    width_shift_range = 0.1,  # randomly shifts images horizontally (fraction of total_width)\n",
    "    height_shift_range = 0.1,  # randomly shifts images vertically (fraction of total-height)\n",
    "    horizontal_flip = True,  # randomly flip images\n",
    "    vertical_flip = False  # randomly flip images\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba51884",
   "metadata": {},
   "source": [
    "## Model defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc544c0",
   "metadata": {},
   "source": [
    "Let's define a simple CNN model with 3 Convolutional layers followed by max-pooling layers. A dropout layer is added after the 3rd maxpool operation to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sequential()\n",
    "model.add(Conv2D(32, 1, padding='same', activation='relu', input_shape=(IMM_SIZE, IMM_SIZE, 1)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, 1, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 1, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.000001)\n",
    "model.compile(optimizer = opt, loss = tf.keras.losses.SparceCategoricalCrossentropy(from_logits = True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fd98d",
   "metadata": {},
   "source": [
    "Now, let's train our model for 2000 epochs. Admittedly, the fitting process is very slow. Therefore, we saved the fitted model to a file. To save time, we will upload the fitted model. If you would like, you can change the parameter fitting to True in our order to refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = False\n",
    "fitting_save = False\n",
    "epochs = 2\n",
    "\n",
    "import pickle\n",
    "\n",
    "if fitting:\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val), shuffle=True)\n",
    "    if fitting_save:\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open('model.json','w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        mode.save_weights('model.h5')\n",
    "        print('Save modelm to disk')\n",
    "        with open('history.pickle','wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        with open('lab.pickle','wb') as f:\n",
    "            pickle.dump(lab, f)\n",
    "# load model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json','r')\n",
    "loaded_model_json=json_file.read()\n",
    "json_file.close()\n",
    "model=model_from_json(loaded_model_json)\n",
    "# load weights into a new model\n",
    "model.load_weights('model.h5')\n",
    "with open('history.pickle','rb') as f:\n",
    "    history=pickle.load(f)\n",
    "print('Loaded model from disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79ee18",
   "metadata": {},
   "source": [
    "## Results evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa6d9b",
   "metadata": {},
   "source": [
    "We will plot out training and validation accuracy along with the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b434738",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=history['accuracy']\n",
    "val_acc=history['val_accuracy']\n",
    "loss=history['loss']\n",
    "val_loss=history['val_loss']\n",
    "\n",
    "epochs_range=range(epochs)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5590a73",
   "metadata": {},
   "source": [
    "We can see the accuracy and training and validation sets is the same. The loss function of validation and training sets is stable. It means that our CNN is fitted well and can be used for classification.\n",
    "\n",
    "We can print out the classification report to see the precision and accuracy using model.predict_classes() and classification_report().\n",
    "\n",
    "Also we can create a confusion matrix. Unfortunetly keras framework does not have plot_confusion_matrix() function. Therefore, we have to create it using pandas and Seaborn.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b44432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "predictions= model.predict_classes(x_val)\n",
    "predictions= predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_val, predictions, target_names=lab.keys()))\n",
    "\n",
    "# Confusion matrix\n",
    "cm=pd.DataFrame(confusion_matrix(y_val, predictions, target_names=lab.keys()))\n",
    "cm.index=['Predicted ' + s for s in lab.keys()]\n",
    "cm.comlumns=['True' + s for s in lab.keys()] \n",
    "print(cm)\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_val, predictions), annot=True,\n",
    "           xticklabels=list(lab.keys()), yticklabels=list(lab.keys()))\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61585502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "z=model.predict_classes(x_train)==y_train\n",
    "scores_train=sum(z+0)/len(z)\n",
    "z=model.predict_classes(x_val)==y_val\n",
    "score_test=sum(z+0)/len(z)\n",
    "print('Training DataSet accuracy : {:.1%}'.format(scores_train), 'Test DataSet accuracy : {: .1%}'.format(score_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
